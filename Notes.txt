-- Pull the image
>> docker pull jupyter/scipy-notebook

-- Run it
>> docker run jupyter/scipy-notebook

-- Map ports (-p) port_on_host:port_in_image
>> docker run -p 8888(or any):8888 jupyter/scipy-notebook

-- Run in detached mode (to use terminal normally) -d
>> docker run -d -p 8888:8888 jupyter/scipy-notebook

-- Find container ID
>> docker ps

-- Find logs to get the token
>> docker logs (imageID)

-- You can refer to the container using the first characters that are unique for it

-- Force killing the container will cause containers to delete all your files and when you start it again, you will start from scratch
-- This code can be only used for emergency or educational purpose
>> docker rm -f (imageID)

-- To make sure your files are saved, you can either mount a local volume from the machine or a docker volume
-- Mounting using a certain directory
-- Find the path with pwd
>> pwd
C:\Users\ahmad.alashmony

>> docker run -v C:\Users\ahmad.alashmony:/home/jovyan/work jupyter/scipy-notebook

-- Mounting local volume (using pwd directly)
>> docker run -v `pwd`:/home/jovyan/work jupyter/scipy-notebook

-- Create a volume
>> docker volume create vol_name
vol_name

-- Mount it
>> docker run -v vol_name:/home/jovyan/work jupyter/scipy-notebook

--------------------------------- Chapter 3 ---------------------------------

-- Superset runs on port 8088 by default
-- Superset has no official docker images, we can use a community image like https://hub.docker.com/r/amancevice/superset/ or https://hub.docker.com/r/tylerfowler/superset
-- I found an offecial image here apache/superset     apache/superset:fb15278-dev
-- Superset takes a while to run, it sets up tables in its database before running
-- In the first run, Superset will require a new secret, generate with `openssl rand -base64 42` and add it to superset_config.py file as `SECRET_KEY = 'YOUR_OWN_RANDOM_GENERATED_SECRET_KEY'`

-- I used the docker compose way instead as per https://superset.apache.org/docs/installation/docker-compose/, here is a video that describes it https://www.youtube.com/watch?v=idCgc1tR1d0
-- The steps as follows:
    * step 2.1: git clone --depth=1  https://github.com/apache/superset.git
    * Step 2.2: cd superset
    * Step 2.3: docker compose -f docker-compose-non-dev.yml up
	* Use the following to run the docker image
		>> docker run -d -p {8088 or any}:8088 {tylerfowler/superset or amancevice/superset}

-- The way by Osama
	* Step 1: Pull the image `docker pull apache/superset`
	* Step 2: Get a random key with `openssl rand -base64 42` -> hNpB/hwykAkPYZcjLxqZwIPHOs2G7cjSYg73s++sqy0p1p2IFr3PiFEV
	* Step 3: Create a container with `docker run -d -p 8080:8088 -e "SUPERSET_SECRET_KEY=hNpB/hwykAkPYZcjLxqZwIPHOs2G7cjSYg73s++sqy0p1p2IFr3PiFEV" --name superset apache/superset`
   	* Step 4: Create username and password with `docker exec -it superset superset fab create --username alashmony --firstname Ahmad --lastname Alashmony --email a.alashmony@itworx.com --password P@ssM0rd`

-- The venv way (Worked with me)
	* Step 1: Create a venv `python -m venv 'superset'`
	* Step 2: Activate it `.\superset\Scripts\activate`
	* Step 3: Download and install superset `python -m pip install apache-superset`
	* Step 4: Get a SECRET_KEY with `openssl rand -base64 42` -> hNpB/hwykAkPYZcjLxqZwIPHOs2G7cjSYg73s++sqy0p1p2IFr3PiFEV
	* Step 5: Add the secret key to the venv:
		* 5.1: Initiate the file `superset_config.py` in a certain path `/config/path`
		* 5.2: Add the required sectret in the file 'SECRET_KEY = 'hNpB/hwykAkPYZcjLxqZwIPHOs2G7cjSYg73s++sqy0p1p2IFr3PiFEV'
		* 5.3: Create an evironment variable: Linux `export SUPERSET_CONFIG_PATH=/config/path/superset_config.py`, Windows `setx SUPERSET_CONFIG_PATH 'C:\config\path\superset_config.py`
	* Step 6: Set the flask app `export FLASK_APP=superset` or for windows `setx FLASK_APP 'C:\path\to\superset(evironment)\Lib\superset'`
	* Step 7: Reload the terminal to read the env variables
	* Step 8: Initiate the DB with `superset db upgrade`
	* Step 9: Create the admin using `superset fab create-admin`, username alashmony, first, last, email, and P@ssM0rd
	* Step 10: Init the application `superset init`
	* Step 11: Run it `superset run -p 8088 --with-threads --reload --debugger`
	* Step 11: Go to http://localhost:8088



-- The default username for superset is "admin" and the default password is "superset", in case of setting up a username and password, use yours


-- To upload a CSV dataset, go to sources, Upload CSV, choose a table name, file, type the delimiter and save. You should see a success message and find the table.
-- SQL Lab is a SQL interface allows you to query the data even without leaving the interface, even if the data is a csv.
-- To defines Dimensions and Measuresm, navigate to Sources -> Tables --> Click the Edit records symbols besides the table name, for the Dimensions, check the "Groupable" and the "Filterable" and Unchkeck the "Sum" 

-- Docker has a tool called docker-compose that collects multiple containers and runs then together (in the same environment) to be able to contact each other
-- Docker-Compose starts multiple containers, uses docker-compose.yml (instead of Dockerfile), and starts with `docker-compose up` instead of `docker run`
-- Docker-compose requires using a YAML file which specifies the desired insfrastructure up-front
-- The docker-compose.yml contains the version and the services to run as follows:
`
version: "3"
services:
	jupyter:
		image: jupyter/scipy-notebook
		container_name: "my_jupyter"
		ports:
			- 8888:8888
	postgres:
		image: postgres
		container_name: "my_postgres"
		ports:
			- 5432:5432
`
-- Now, we can use >> `docker-compose up` to run both images and interact between them
-- The magic comes when you create a connection from one container to the other, use the container name as the host name you need to connect to. It is used instead of the IP or the hostname of the DB instance.
-- You can inject a file in a specific folder as below:
	* Create a folder in the same directory as the docker-compose.yml called 'scripts'
	* Create a file in that folder called '01_my_script.sh' 
	* Add the file to the image (in a folder called my_folder) using the docker-compose.yml as below:
`
version: "3"
services:
	jupyter:
		image: jupyter/scipy-notebook
		container_name: "my_jupyter"
		ports:
			- 8888:8888
		volumes:                          	<- This line injects the files as a volume
			- ./scripts/:/my_folder/		<- This line says to inject all files in the 'scripts' folder into 'my_folder' folder in the image
	postgres:
		image: postgres
		container_name: "my_postgres"
		ports:
			- 5432:5432
`
-- To add an environment variable (for example, to set JAVA_HOME to /etc/java) to the image use the following:
`
version: "3"
services:
	jupyter:
		image: jupyter/scipy-notebook
		container_name: "my_jupyter"
		ports:
			- 8888:8888
		volumes:
			- ./scripts/:/my_folder/
		environment:					<- This line starts setting the environment variables 
			- JAVA_HOME=/etc/java/		<- This one sets JAVA_HOME to be equal to '/etc/java'
	postgres:
		image: postgres
		container_name: "my_postgres"
		ports:
			- 5432:5432
`

-- Machine Learning models are not tabular data to store in a structured database
-- Minio is an object store which is mostly like a key-value store for any type of files
--  To run a menio container run the following commande:
>> docker run -p 9000:9000 -t minio/minio server /data

	* The `-t` flag means to attach the console to the container to extract the secret keys which is a requirement for the minio 
	* The `server /data` means to store data we upload to the server in the '/data' folder in the image

-- Running a docker for minio will require a volume also, create with
>> docker volume create mn_vol

-- Now rerun it with 
>> docker run -d -p 9000:9000 -v mn_vol:/data -t minio/minio server /data 
	* It seems that the port for webUI is changing randomly, i might need to try it in the docker-compose way.

-- To install the minio library in python, just install minio with pip
>> pip install Minio

-- To interact with minio, you should connect to minio server as follows (the server name is 'minio' if used the docker-compose and localhost if used local machine):
`
from minio import Minio
minio_client = Minio (endpoint = minio:9000, access_key = '', secret_key = '', secure = False)
`

-- Now, you can create a bucket (folder)
`
minio_client.make_bucket('my-folder')
`

-- You can also write an object (a file called sample_file.txt) into the bucket:
`
minio_client.fput_object(bucket_name = 'my-folder', object_name = 'sample_file.txt', file_path = 'C:\\sample_file.txt')
`

-- To read a file, simply do it:
`
minio_client.get_object('my-folder', 'sample_file.txt').data.decode()
`
	* The '.data' attribute and the'.decode()' method are used to print what is written in the file

-- minio did not accept the undetscores '_' in a bucket name

-- API Star is a framework to create API end points with automated documentation

-- API Star consists of:
	* The Handler which is a python function that returns the dictionary (the JSON response object)
	* The Routes which is a list of routes to call the API end points
	* The App that contains Handlers and Routes 

-- To define an end point, just import and define them
`
from apistar import App, Route

# Define the handler (function)
def welcome_me(name = None):
	if name:
		return {'body':f'Welcome {name}!'}
	else:
		return {'body': 'Welcome dear!'}

# Define the routes as a list of routes
my_routes = [Route('/', method='GET', handler = welcome_me)]

# Define the App
app = App(routes = my_routes)

# Run the App
if __name__ == "__main__":
	app.serve('127.0.0.1', 5000, debug = True)
`

-- Run the file.py with the normal python shell
>> python file.py

-- Call the API with curl 
>> curl http://localhost:5000 
Welcome dear!

>> curl http://localhost:5000/?name\=Ahmad
Welcome Ahmad!

-- To create a docker image for API Star, create a docker file as follows:
`
From python:3-alpine

WORKDIR /usr/src/app 				<-- This line defines that commands will run in this directory

Run pip --no-cashe-dir install \	<-- The --no-cashe-dir removes cashe used by pip after installation
	apistar==0.5.41

EXPOSE 8000
`

-- Then create the docker container and name it apistar with the following:
>> docker build . -t apistar		<-- the -t tags/flags the image with the name 'apistar'

-- Run the container
>> docker run -d -p 5000:8000 -v `pwd`:/usr/src/app apistar python app.py

-- Test the container 
>> http localhost:5000
.....
{
	'body':'Welcome dear!'
}

-- The course shows how to install app libraries needed for the whole ML model and the API star. dockerfile can be found in ch_06/example_enhanced/
-- Can use docker to build it with the following noting that it will take some time (10 to 20 minutes) to install the required libraries
>> docker build . -t apistar:latest



-- Airflow is a sophisticated scheduling framework that offers a lot of flixibility, it also can provide diagnostic reports on the job

-- Airflow consists of:
	* Tasks: an atomic command; Python code, bash script, http request...etc. Tasks cannot run directly, you have to rep in a DAC.
	* DAG (Directed Acyclic Graph): A container for tasks that orchestinates them (Like AWS orchestinator)

-- Examples of creating DAGs in the example and example-enriched folders	

-- You might need addiotoinal components to the Data Science projects like:
	* Cashe (In- memory) components: Radis
	* Graph DB: Neo4J
	* Webserver: NGINX
	* Monitoring: Prometheus
	* Text search: Solr 

-- BONUS: Cookiecutter Template
	* Bonus Section
	* Welcome to this little bonus. In case you liked the stack presented in this video I have a little present for you :-)
	* If you would like to start a new project it is tedious to set the whole docker structure up (and not really related to any meaningful data science content). That's why I create a Cookiecutter Template for it.

-- Cookiecutter Template
	* Cookiecutter is a command line tool (based on Python) that lets you generate new projects from a boilerplate. 
	* Opposed to many other solutions (e.g. git fork), cookiecutter deeply looks for parameter you've specified and adjusts them also inside of files.

-- If you would like to use this Cookiecutter template, just install cookiecutter via
>> pip install cookiecutter

-- and follow the instruction in this repository:

https://github.com/jgoerner/data-science-stack-cookiecutter

-- If you have any questions or remarks, feel free to use the Issue section of this repository.